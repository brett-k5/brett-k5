# Hi, I'm Brett ğŸ‘‹

I'm a machine learning engineer with a strong foundation in statistics, logic, and data-driven problem solving. I enjoy building well-engineered solutions using real-world data and delivering measurable impact through model performance and clarity.

## ğŸ” Projects

### ğŸ¦ Customer Churn Prediction | XGBoost, SHAP, Feature Engineering  
Built a predictive model for customer churn at an insurance company using customer demographics, service usage patterns, and contract information.  
- Achieved 0.90 ROC AUC by carefully engineering features and addressing data leakage.
- Used SHAP for model interpretability to identify key churn drivers.

### ğŸ’¬ Sentiment Classification | BERT, Transformers  
Embedded text using a pre-trained BERT model and trained a classifier on sentimentlabels.  
- Included end-to-end data preprocessing, tokenization, and model training.
- Achieved roc_auc of .94 while handling class imbalance and text variability.

### ğŸ§  Age Prediction from Faces | CNN, Keras  
Built a deep learning model to estimate real-world age from facial images using the APPA-REAL dataset.  
- Focused on image preprocessing, model training, and validation performance tracking.
- Used callbacks like `ModelCheckpoint` to retain best model weights based on validation MAE.
- Acheived MAE of 8.5 on test set. 

### ğŸš• Time Series Forecasting | XGBoost, Lag Features, Cyclic Encoding  
Forecasted hourly taxi demand using time-based features and gradient boosting models.  
- Engineered cyclic features (hour-of-day, day-of-week) and lag-based variables.
- Compared predictions against a naive "last value" baseline and significantly reduced MAE (58.81 to 35.80) 

## âš™ï¸ Tools & Libraries
Python, Pandas, NumPy, scikit-learn, XGBoost, LightGBM, CatBoost, TensorFlow, Keras, Hugging Face Transformers, Matplotlib, Seaborn

## ğŸ“Œ Interests
Gradient boosting, deep learning, NLP, time series forecasting, and model interpretability
